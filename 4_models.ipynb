{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Model\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from settings import *\n",
    "from PIL import Image\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original leNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_original_model():\n",
    "    input_data= layers.Input(shape=(32,32,3))\n",
    "    conv_1=layers.Conv2D(filters=6, kernel_size=5,padding=\"VALID\",strides=(1, 1),activation=\"relu\")(input_data)\n",
    "    P1=layers. MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid')(conv_1)\n",
    "    \n",
    "    conv_2=layers.Conv2D(filters=16, kernel_size=5,padding=\"VALID\",strides=(1, 1),activation=\"relu\")(P1)\n",
    "    P2=layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid')(conv_2)\n",
    "    \n",
    "    F2=layers.Flatten()(P2)\n",
    "    \n",
    "    \n",
    "    D1=layers.Dense(units=120, activation='relu')(F2)\n",
    "    \n",
    "    D2=layers.Dense(units=84, activation='relu')(D1)\n",
    "  \n",
    "    D3=layers.Dense(units=43, activation = 'softmax')(D2)\n",
    "    \n",
    "    model = Model(inputs=input_data, outputs=D3)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model=create_original_model()\n",
    "plot_model(original_model, to_file='original_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changed architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_changed_model():\n",
    "    input_data= layers.Input(shape=(32,32,3))\n",
    "    conv_1=layers.Conv2D(filters=6, kernel_size=5,padding=\"VALID\",strides=(1, 1),activation=\"relu\")(input_data)\n",
    "    P1=layers. MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid')(conv_1)\n",
    "    \n",
    "    conv_2=layers.Conv2D(filters=16, kernel_size=5,padding=\"VALID\",strides=(1, 1),activation=\"relu\")(P1)\n",
    "    P2=layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid')(conv_2)\n",
    "    \n",
    "    F2=layers.Flatten()(P2)\n",
    "    F1=layers.Flatten()(P1)\n",
    "    Merge=concatenate([F1, F2])\n",
    "    \n",
    "    \n",
    "    D1=layers.Dense(units=120, activation='relu')(Merge)\n",
    "    \n",
    "    D2=layers.Dense(units=84, activation='relu')(D1)\n",
    "  \n",
    "    D3=layers.Dense(units=43, activation = 'softmax')(D2)\n",
    "    \n",
    "    model = Model(inputs=input_data, outputs=D3)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_model=create_changed_model()\n",
    "plot_model(changed_model, to_file='Changed_architecture_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout on fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define keras model\n",
    "def create_dropout_fully_connected_model():\n",
    "    \n",
    "    input_data= layers.Input(shape=(32,32,3))\n",
    "    conv_1=layers.Conv2D(filters=6, kernel_size=5,padding=\"VALID\",strides=(1, 1),activation=\"relu\")(input_data)\n",
    "    P1=layers. MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid')(conv_1)\n",
    "    \n",
    "    conv_2=layers.Conv2D(filters=16, kernel_size=5,padding=\"VALID\",strides=(1, 1),activation=\"relu\")(P1)\n",
    "    P2=layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid')(conv_2)\n",
    "    \n",
    "    F2=layers.Flatten()(P2)\n",
    "    F1=layers.Flatten()(P1)\n",
    "    Merge=concatenate([F1, F2])\n",
    "    \n",
    "    Merge_dropout=layers.Dropout(rate=0.1)(Merge)\n",
    "    \n",
    "    D1=layers.Dense(units=120, activation='relu')(Merge_dropout)\n",
    "    D1_dropout= layers.Dropout(rate=0.3)(D1)\n",
    "    \n",
    "    D2=layers.Dense(units=84, activation='relu')(D1_dropout)\n",
    "    D2_dropout=layers.Dropout(rate=0.5)(D2)\n",
    "    \n",
    "    D3=layers.Dense(units=43, activation = 'softmax')(D2_dropout)\n",
    "    \n",
    "    model = Model(inputs=input_data, outputs=D3)\n",
    "        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_fully_connected_model=create_dropout_fully_connected_model()\n",
    "plot_model(dropout_fully_connected_model, to_file='Dropout_fully_connected _model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout on fully connected and convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dropout_fully_connected_convolutional_model():\n",
    "    \n",
    "    input_data= layers.Input(shape=(32,32,3))\n",
    "    conv_1=layers.Conv2D(filters=6, kernel_size=5,padding=\"VALID\",strides=(1, 1),activation=\"relu\")(input_data)\n",
    "    P1=layers. MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid')(conv_1)\n",
    "    P1_dropout= layers.Dropout(rate=0.3)(P1)\n",
    "    \n",
    "    conv_2=layers.Conv2D(filters=16, kernel_size=5,padding=\"VALID\",strides=(1, 1),activation=\"relu\")(P1_dropout)\n",
    "    P2=layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid')(conv_2)\n",
    "    P2_dropout= layers.Dropout(rate=0.5)(P2)\n",
    "    \n",
    "    F2=layers.Flatten()(P2_dropout)\n",
    "    F1=layers.Flatten()(P1_dropout)\n",
    "    Merge=concatenate([F1, F2])\n",
    "    \n",
    "    Merge_dropout=layers.Dropout(rate=0.1)(Merge)\n",
    "    D1=layers.Dense(units=120, activation='relu')(Merge_dropout)\n",
    "    D1_dropout= layers.Dropout(rate=0.3)(D1)\n",
    "    \n",
    "    D2=layers.Dense(units=84, activation='relu')(D1_dropout)\n",
    "    D2_dropout=layers.Dropout(rate=0.5)(D2)\n",
    "    \n",
    "    D3=layers.Dense(units=43, activation = 'softmax')(D2_dropout)\n",
    "    \n",
    "    model = Model(inputs=input_data, outputs=D3)\n",
    "        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_fully_connected_convolutional_model=create_dropout_fully_connected_convolutional_model()\n",
    "plot_model(dropout_fully_connected_convolutional_model, to_file='Dropout_fully_connected_convolutional_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GTSRB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train,X_valid,Y_valid=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_hot=keras.utils.to_categorical(Y_train,num_classes=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=optimizers.Adam(lr=0.009, beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configures the models for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.compile(optimizer=\"adam\",\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "changed_model.compile(optimizer=\"adam\",\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "dropout_fully_connected_model.compile(optimizer=\"adam\",\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "dropout_fully_connected_convolutional_model.compile(optimizer=\"adam\",\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training of Original model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# Fit model on training data')\n",
    "original_model_history = original_model.fit(X_train, Y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=100,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_split = 0.2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model and weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.save(\"E:\\\\NNProject\\\\Models_and_weights\\\\original_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training of Changed model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# Fit model on training data')\n",
    "changed_model_history = changed_model.fit(X_train, Y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=100,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_split = 0.2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model and weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_model.save(\"E:\\\\NNProject\\\\Models_and_weights\\\\changed_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training of Dropout fully connected model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('# Fit model on training data')\n",
    "dropout_fully_connected_model_history = dropout_fully_connected_model.fit(X_train, Y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=100,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_split = 0.2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model and weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_fully_connected_model.save(\"E:\\\\NNProject\\\\Models_and_weights\\\\dropout_fully_connected_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training of Dropout_fully_connected_convolutional_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# Fit model on training data')\n",
    "dropout_fully_connected_convolutional_model_history = dropout_fully_connected_convolutional_model.fit(X_train, Y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=100,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_split = 0.2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model and weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_fully_connected_convolutional_model.save(\"E:\\\\NNProject\\\\Models_and_weights\\\\dropout_fully_connected_convolutional_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation of Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_score = original_model.evaluate(X_valid, Y_valid, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_model_score = changed_model.evaluate(X_valid, Y_valid, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_fully_connected_model_score = dropout_fully_connected_model.evaluate(X_valid, Y_valid, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_fully_connected_convolutional_model_score = dropout_fully_connected_convolutional_model.evaluate(X_valid, Y_valid, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_acc      = original_model_history.history[     'acc' ]\n",
    "original_model_val_acc  = original_model_history.history[ 'val_acc' ]\n",
    "original_model_loss     = original_model_history.history[    'loss' ]\n",
    "original_model_val_loss = original_model_history.history['val_loss' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_model_acc      = changed_model_history.history[     'acc' ]\n",
    "changed_model_val_acc  = changed_model_history.history[ 'val_acc' ]\n",
    "changed_model_loss     = changed_model_history.history[    'loss' ]\n",
    "changed_model_val_loss = changed_model_history.history['val_loss' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_fully_connected_model_acc      = dropout_fully_connected_model_history.history[     'acc' ]\n",
    "dropout_fully_connected_model_val_acc  = dropout_fully_connected_model_history.history[ 'val_acc' ]\n",
    "dropout_fully_connected_model_loss     = dropout_fully_connected_model_history.history[    'loss' ]\n",
    "dropout_fully_connected_model_val_loss = dropout_fully_connected_model_history.history['val_loss' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_fully_connected_convolutional_model_acc      = dropout_fully_connected_convolutional_model_history.history[     'acc' ]\n",
    "dropout_fully_connected_convolutional_model_val_acc  = dropout_fully_connected_convolutional_model_history.history[ 'val_acc' ]\n",
    "dropout_fully_connected_convolutional_model_loss     = dropout_fully_connected_convolutional_model_history.history[    'loss' ]\n",
    "dropout_fully_connected_convolutional_model_val_loss = dropout_fully_connected_convolutional_model_history.history['val_loss' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs   = range(len(changed_model_acc))\n",
    "plt.plot  ( epochs,     original_model_val_loss ,'b' )\n",
    "plt.plot  ( epochs, changed_model_val_loss,'r' )\n",
    "plt.plot  ( epochs, dropout_fully_connected_model_val_loss,'g' )\n",
    "plt.plot  ( epochs, dropout_fully_connected_convolutional_model_val_loss ,'y' )\n",
    "plt.xlabel('epochs', fontsize=16)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "plt.title ('validation loss')\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.plot  ( epochs,     original_model_val_acc ,'b' )\n",
    "plt.plot  ( epochs, changed_model_val_acc,'r' )\n",
    "plt.plot  ( epochs, dropout_fully_connected_model_val_acc,'g' )\n",
    "plt.plot  ( epochs, dropout_fully_connected_convolutional_model_val_acc ,'y' )\n",
    "plt.xlabel('epochs', fontsize=16)\n",
    "plt.ylabel('accuracy', fontsize=16)\n",
    "plt.title ('validation accuracy'   )\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#path to the wanted h5 file\n",
    "h5_path=\"\"\n",
    "model = load_model(h5_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=\"E:\\\\NNProject\\\\test_data\"\n",
    "M= len([name for name in os.listdir(folder_path) if os.path.join(folder_path,name).endswith(\".ppm\")])\n",
    "ncols=5\n",
    "columns=1\n",
    "rows=0\n",
    "nrows=(M//5)+1\n",
    "\n",
    "img_width,img_height=32,32\n",
    "fig,axes = plt.subplots(nrows, ncols, figsize=(100,100))\n",
    "i=0\n",
    "#plt.rcParams.update({'font.size': 12})\n",
    "print(\"columns : \",columns,\"   rows:\",rows,\"    i:\",i )\n",
    "for img in os.listdir(folder_path):\n",
    "    img_path = os.path.join(folder_path, img)\n",
    "    img = Image.open(img_path)\n",
    "    #print(img.size)\n",
    "    img = img.resize((img_width,img_height))\n",
    "    #print(img.size)\n",
    "    img.save(img_path)\n",
    "    image = mpimg.imread(img_path)\n",
    "    im = np.array(image).reshape(1, img_width,img_height,3)\n",
    "    \n",
    "    \n",
    "    y=dropout_fully_connected_model.predict(im)\n",
    "    \n",
    "    index=np.where(y[0]==np.amax(y))\n",
    "    label=index[0][0]\n",
    "    #print(label)\n",
    "    name= get_name_by_index(label)\n",
    "    #print(\"i:\",i,\"     rows:\",rows)\n",
    "    if(i >= 5):\n",
    "        #print(\"lllllllll\")\n",
    "        i=0\n",
    "        rows+=1\n",
    "    axes[rows,i].imshow(image)\n",
    "    #need to incease font size\n",
    "    \n",
    "    axes[rows,i].set_xlabel(name)\n",
    "   #or\n",
    "    #axes[rows,i].set_title(name)\n",
    "    i+=1\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers[1:13]]\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "print(type(activation_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"E:\\\\NNProject\\\\GTSRB_Final_Test_Images\\\\00011.ppm\"\n",
    "\n",
    "img = Image.open(img_path)\n",
    "#print(img.size)\n",
    "img = img.resize((32,32))\n",
    "#print(img.size)\n",
    "img.save(img_path)\n",
    "image = mpimg.imread(img_path)\n",
    "ima = np.array(image).reshape(1, 32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_model.predict(ima) # Returns a list of five Numpy arrays: one array per layer activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_activation = activations[0]\n",
    "#print(first_layer_activation.shape)\n",
    "plt.matshow(first_layer_activation[0,:,:,4], cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = []\n",
    "for layer in model.layers[:4]:\n",
    "    #if(layer.name is not 'concatenate_2'):\n",
    "    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n",
    "    \n",
    "images_per_row = 6\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
    "    n_features = layer_activation.shape[-1] # Number of features in the feature map\n",
    "    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
    "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "    #print (layer_name)\n",
    "    #print(type(display_grid))\n",
    "    for col in range(n_cols): \n",
    "       # print(col)\n",
    "        for row in range(images_per_row):\n",
    "            #scale * display_grid.shape[0] == 0\n",
    "            print(layer_activation.shape)\n",
    "            channel_image = layer_activation[0, :, :, (col * images_per_row + row)]\n",
    "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "   \n",
    "    plt.title(layer_name)\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
